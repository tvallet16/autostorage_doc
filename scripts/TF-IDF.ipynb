{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93cdbfbd-c9b6-442f-8bd5-ea341a40f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from textScrapper import TextScrapper\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string\n",
    "import re\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bcc8169-c546-4ea7-88b1-37627786d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemple path file ../data/sncf_vinci_ifg_62/00086570-002601E-B007-20220323-R-VEN.pdf\n",
    "class fileTextManager():\n",
    "    \n",
    "    def __init__ (self):\n",
    "        self.current_DB= \"sncf_vinci_ifg_62\"\n",
    "        \n",
    "        \n",
    "    def cleanText(self, text):\n",
    "        char=\"abcdefghijklmnopqrstuvwxyzéèàçôùëïüûâùäîê-. \"\n",
    "        lowercaseText= text[0].lower()\n",
    "        cleanText = \"\"\n",
    "        for letter in lowercaseText:\n",
    "            if letter in char:\n",
    "                cleanText = cleanText + letter\n",
    "        return cleanText\n",
    "\n",
    "    def getExtension(self, file):\n",
    "        filename, file_extension = os.path.splitext(file)\n",
    "        return file_extension\n",
    "    \n",
    "    \n",
    "    def getFileText(self, file):\n",
    "        textScrap = TextScrapper()\n",
    "        typeDocumentNumber=[]\n",
    "        match file[\"mimeType\"]:\n",
    "            case 'application/pdf':\n",
    "                try:\n",
    "                    fullText = textScrap.getTextPdf(f\"/Volumes/DD Thibault/sncf_DB/doc/{self.current_DB}/{file['KidFile']}\")\n",
    "                except:\n",
    "                    fullText = None\n",
    "                fullText = self.cleanText(fullText)\n",
    "                return fullText\n",
    "            case 'application/vnd.ms-excel':\n",
    "                \n",
    "                fullText = textScrap.getTextExcel(f\"/Volumes/DD Thibault/sncf_DB/doc/{self.current_DB}/{file['KidFile']}\")\n",
    "                return fullText\n",
    "\n",
    "            case 'application/vnd.openxmlformats-officedocument.wordprocessingml.document':\n",
    "                fullText = textScrap.getTextWord(f\"/Volumes/DD Thibault/sncf_DB/doc/{self.current_DB}/{file['KidFile']}\")\n",
    "                return fullText\n",
    "            \n",
    "            #case 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet':\n",
    "                #fullText = textScrap.getTextWord(f\"/Volumes/DD Thibault/sncf_DB/doc/engie/{file['KidFile']}\")\n",
    "                #return fullText\n",
    "            #case 'image/jpeg':\n",
    "                #return 2\n",
    "\n",
    "            case _:\n",
    "                return 'document type not supported'\n",
    "                #raise ValueError('document type not supported')   # 0 is the default case if x is not found\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a017b4e-770c-434b-9f11-77964d4772ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2470\n",
      "fileName                   4129K-B006-GAZ.pdf\n",
      "KidDocumentClass       K_NODE;cATERTETANCHGAZ\n",
      "fileSize                               799744\n",
      "mimeType                      application/pdf\n",
      "owner               PROFIL;0001015M0324LV55BY\n",
      "creationDate              2021-02-04 14:58:55\n",
      "KidFile             K_NODE;0022805M2U19ARDWQT\n",
      "Name: 4, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2470it [06:58,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_NODE;cATERTETANCHGAZ\n",
      "K_NODE;cATERTETANCHGAZ\n",
      "51\n",
      "K_NODE;cATERTRAMON\n",
      "K_NODE;cATERTRAMON\n",
      "122\n",
      "K_NODE;cATERTFLUIDO\n",
      "K_NODE;cATERTFLUIDO\n",
      "137\n",
      "K_NODE;cATERTDISCO\n",
      "K_NODE;cATERTDISCO\n",
      "124\n",
      "K_NODE;cATERTDEBAIR\n",
      "K_NODE;cATERTDEBAIR\n",
      "50\n",
      "K_NODE;cATERDESENFNAT\n",
      "K_NODE;cATERDESENFNAT\n",
      "61\n",
      "K_NODE;cATERTRIA\n",
      "K_NODE;cATERTPOT\n",
      "K_NODE;cATERTPOT\n",
      "53\n",
      "K_NODE;cATERTEXG\n",
      "K_NODE;cATERTPAU\n",
      "K_NODE;cATERTPAU\n",
      "1756\n",
      "K_NODE;cATERTSSI\n",
      "K_NODE;cATERTSSI\n",
      "91\n",
      "K_NODE;cATERTATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "csvPath = \"/Volumes/DD Thibault/sncf_DB/doc/sncf_vinci_ifg_62.csv\"\n",
    "scrapper = TextScrapper()\n",
    "csv = scrapper.getTextCsv(csvPath)\n",
    "dictPdfWord=[]\n",
    "fileManager = fileTextManager()\n",
    "print(len(csv))\n",
    "print(csv.iloc[4])\n",
    "orderedCorpus = {}\n",
    "dictType={}\n",
    "\n",
    "for row in tqdm((csv.iloc)):\n",
    "    if row[\"mimeType\"] == \"application/pdf\" or row[\"mimeType\"] == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\":\n",
    "        contentFile = fileManager.getFileText(row)\n",
    "        if row['KidDocumentClass'] in dictType :\n",
    "            dictType[row['KidDocumentClass']].append(contentFile)\n",
    "        else:\n",
    "            dictType[row['KidDocumentClass']]=[]\n",
    "            dictType[row['KidDocumentClass']].append(contentFile)\n",
    "        #TODO charge content by batch\n",
    "data = {}\n",
    "\n",
    "for key in dictType:\n",
    "    if len(dictType[key]) >= 20:\n",
    "        if key in data:\n",
    "            data[key][\"data\"], data[key][\"test\"]= train_test_split(dictType[key],test_size=0.20)\n",
    "        else :\n",
    "            data[key] = {\"data\":[], \"test\":[]}\n",
    "            data[key][\"data\"], data[key][\"test\"]= train_test_split(dictType[key],test_size=0.20)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b055193d-976f-419d-b497-35803f967e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_NODE;cATERTETANCHGAZ\n",
      "K_NODE;cATERTRAMON\n",
      "K_NODE;cATERTFLUIDO\n",
      "K_NODE;cATERTDISCO\n",
      "K_NODE;cATERTDEBAIR\n",
      "K_NODE;cATERDESENFNAT\n",
      "K_NODE;cATERTPOT\n",
      "K_NODE;cATERTPAU\n",
      "K_NODE;cATERTSSI\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7719515e-2acf-460e-a753-2c10d5c27d79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 9/9 [00:04<00:00,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rapport', 'dintervention', 'contrôle', 'de', 'létanchéité', 'du', 'rés', 'eau', 'gaz', 'n', 'date', 'du', 'rapport', 'n', 'de', 'fiche', 'client', 'sncf', 'site', 'nbbrigade', 'voie', 'adresse', 'malesherbes', 'gare', 'malesherbes', 'responsable', 'affaire', 'mustapha', 'khallouk', 'responsable', 'technique', 'certificat', 'd', 'étanchéité', 'je', 'soussigné', 'abdelouahid', 'daoudi', 'avoir', 'effectué', 'le', 'contrôle', 'de', 'létanchéité', 'du', 'réseau', 'gaz', 'entre', 'le', 'point', 'de', 'livraison', 'du', 'site', 'à', 'léquipement', 'de', 'production', '.'], ['statut', 'de', 'létanchéité', 'du', 'réseau', 'de', 'gaz', 'étanche', 'observation', 'cuve', 'pour', 'aiguille', 'non', 'étanche', 'signalé', 'à', 'm.guymar', 'technicien', 'vinci', 'facilities', 'nom', 'et', 'signature', 'signature', 'client', 'abdelouahid', 'daoudi', 'abdelouahid.daoudivinci', '-facilities.com']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dictType' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                 tokenizeDict[types][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(wordTokenize)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizeDict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK_NODE;cATERTETANCHGAZ\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m dictType\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dictType' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizeDict = {}\n",
    "for types in tqdm(data):\n",
    "    for file in data[types][\"data\"]:\n",
    "        if types in tokenizeDict:\n",
    "            if isinstance(file, str) and file != \"\":\n",
    "                sentTokenize = sent_tokenize(file)\n",
    "                wordTokenize = []\n",
    "                for sent in sentTokenize:\n",
    "                    wordTokenize.append(word_tokenize(sent))\n",
    "                tokenizeDict[types][\"data\"].append(wordTokenize)\n",
    "        else:\n",
    "            tokenizeDict[types]={\"data\":[], \"test\":[]}\n",
    "            if isinstance(file, str) and file != \"\":\n",
    "                sentTokenize = sent_tokenize(file)\n",
    "                wordTokenize = []\n",
    "                for sent in sentTokenize:\n",
    "                    wordTokenize.append(word_tokenize(sent))\n",
    "                tokenizeDict[types][\"data\"].append(wordTokenize)\n",
    "                \n",
    "    for file in data[types][\"test\"]:\n",
    "        if types in tokenizeDict:\n",
    "            if isinstance(file, str) and file != \"\":\n",
    "                sentTokenize = sent_tokenize(file)\n",
    "                wordTokenize = []\n",
    "                for sent in sentTokenize:\n",
    "                    wordTokenize.append(word_tokenize(sent))\n",
    "                tokenizeDict[types][\"test\"].append(wordTokenize)\n",
    "        \n",
    "print(tokenizeDict[\"K_NODE;cATERTETANCHGAZ\"][\"test\"][0])\n",
    "del dictType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2cae90d-54a3-44a8-80c2-5002d32da5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████████████████▋                                                     | 3/9 [00:00<00:00, 17.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_NODE;cATERTETANCHGAZ\n",
      "K_NODE;cATERTRAMON\n",
      "K_NODE;cATERTFLUIDO\n",
      "K_NODE;cATERTDISCO\n",
      "K_NODE;cATERTDEBAIR\n",
      "K_NODE;cATERDESENFNAT\n",
      "K_NODE;cATERTPOT\n",
      "K_NODE;cATERTPAU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00,  9.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_NODE;cATERTSSI\n",
      "{'-': 0, '.': 1, 'adresse': 2, 'affaire': 3, 'avenue': 4, 'avoir': 5, 'certificat': 6, 'client': 7, 'contrôle': 8, 'd': 9, 'date': 10, 'de': 11, 'dintervention': 12, 'du': 13, 'eau': 14, 'effectué': 15, 'entre': 16, 'fiche': 17, 'gaz': 18, 'je': 19, 'jeremy': 20, 'khallouk': 21, 'le': 22, 'livraison': 23, 'léquipement': 24, 'létanchéité': 25, 'mustapha': 26, 'n': 27, 'orleans': 28, 'paris': 29, 'point': 30, 'production': 31, 'puard': 32, 'rapport': 33, 'rbbatiment': 34, 'responsable': 35, 'rés': 36, 'réseau': 37, 'site': 38, 'sncf': 39, 'soussigné': 40, 'technique': 41, 'voyageurs': 42, 'à': 43, 'étanchéité': 44, '-facilities.com': 45, 'et': 46, 'facilities': 47, 'jeremy.puardvinci': 48, 'nom': 49, 'observation': 50, 'signature': 51, 'statut': 52, 'technicien': 53, 'vinci': 54, 'étanche': 55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "associativeDictionary = {}\n",
    "for types in tqdm(tokenizeDict):\n",
    "    print(types)\n",
    "    for file in tokenizeDict[types][\"data\"]:\n",
    "        if types in associativeDictionary:\n",
    "            associativeDictionary[types][\"data\"].append(gensim.corpora.Dictionary(file))\n",
    "        else:\n",
    "            associativeDictionary[types]={\"data\":[], \"test\":[]}\n",
    "            associativeDictionary[types][\"data\"].append(gensim.corpora.Dictionary(file))\n",
    "    for file in tokenizeDict[types][\"test\"]:\n",
    "        associativeDictionary[types][\"test\"].append(gensim.corpora.Dictionary(file))\n",
    "        \n",
    "print(associativeDictionary[\"K_NODE;cATERTETANCHGAZ\"][\"test\"][8].token2id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df6c5f5d-1286-4dad-acbf-d64d4757fe7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'abdelouahid': 1, 'adresse': 2, 'affaire': 3, 'avoir': 4, 'certificat': 5, 'client': 6, 'contrôle': 7, 'd': 8, 'daoudi': 9, 'date': 10, 'de': 11, 'dintervention': 12, 'du': 13, 'eau': 14, 'effectué': 15, 'entre': 16, 'fiche': 17, 'gare': 18, 'gaz': 19, 'je': 20, 'khallouk': 21, 'le': 22, 'livraison': 23, 'léquipement': 24, 'létanchéité': 25, 'malesherbes': 26, 'mustapha': 27, 'n': 28, 'nbbrigade': 29, 'point': 30, 'production': 31, 'rapport': 32, 'responsable': 33, 'rés': 34, 'réseau': 35, 'site': 36, 'sncf': 37, 'soussigné': 38, 'technique': 39, 'voie': 40, 'à': 41, 'étanchéité': 42, '-facilities.com': 43, 'abdelouahid.daoudivinci': 44, 'aiguille': 45, 'cuve': 46, 'et': 47, 'facilities': 48, 'm.guymar': 49, 'nom': 50, 'non': 51, 'observation': 52, 'pour': 53, 'signalé': 54, 'signature': 55, 'statut': 56, 'technicien': 57, 'vinci': 58, 'étanche': 59}\n",
      "K_NODE;cATERTETANCHGAZ\n",
      "K_NODE;cATERTRAMON\n",
      "K_NODE;cATERTFLUIDO\n",
      "K_NODE;cATERTDISCO\n",
      "K_NODE;cATERTDEBAIR\n",
      "K_NODE;cATERDESENFNAT\n",
      "K_NODE;cATERTPOT\n",
      "K_NODE;cATERTPAU\n",
      "K_NODE;cATERTSSI\n",
      "{'.': 0, 'client': 1, 'contrôle': 2, 'date': 3, 'de': 4, 'dintervention': 5, 'du': 6, 'eau': 7, 'fiche': 8, 'gaz': 9, 'létanchéité': 10, 'n': 11, 'nbb.v': 12, 'rapport': 13, 'rés': 14, 'site': 15, 'sncf': 16, 'adresse': 17, 'affaire': 18, 'avoir': 19, 'certificat': 20, 'd': 21, 'effectué': 22, 'entre': 23, 'gare': 24, 'je': 25, 'jeremy': 26, 'khallouk': 27, 'la': 28, 'le': 29, 'livraison': 30, 'léquipement': 31, 'malesherbes': 32, 'mustapha': 33, 'place': 34, 'point': 35, 'production': 36, 'puard': 37, 'responsable': 38, 'réseau': 39, 'soussigné': 40, 'technique': 41, 'à': 42, 'étanchéité': 43, '-facilities.com': 44, 'et': 45, 'facilities': 46, 'jeremy.puardvinci': 47, 'nom': 48, 'observation': 49, 'signature': 50, 'statut': 51, 'technicien': 52, 'vinci': 53, 'étanche': 54}\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 2), (5, 1), (6, 2), (7, 1), (8, 1), (9, 1), (10, 1), (11, 2), (12, 1), (13, 2), (14, 1), (15, 1), (16, 1)], [(0, 1), (2, 1), (4, 4), (6, 2), (9, 1), (10, 1), (15, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (29, 2), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 2), (38, 2), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1)], [(1, 1), (4, 2), (6, 1), (9, 1), (10, 1), (26, 1), (37, 1), (39, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 2), (51, 1), (52, 1), (53, 1), (54, 1)]]\n"
     ]
    }
   ],
   "source": [
    "bowDict = {}\n",
    "#TODO -----> remove flat_list because i think than next step has need list of list and not flat list\n",
    "\n",
    "\n",
    "for types in tokenizeDict:\n",
    "    print(types)\n",
    "    i=0\n",
    "    j=0\n",
    "    for file in tokenizeDict[types][\"data\"]:\n",
    "        tampFile = []\n",
    "        for sentence in file:\n",
    "            if types in bowDict:\n",
    "                tampFile.append(associativeDictionary[types][\"data\"][i].doc2bow(sentence))\n",
    "            else:\n",
    "                bowDict[types]={\"data\":[], \"test\":[]}\n",
    "                tampFile.append(associativeDictionary[types][\"data\"][i].doc2bow(sentence))\n",
    "        bowDict[types][\"data\"].append(tampFile)\n",
    "    for file in tokenizeDict[types][\"test\"]:\n",
    "        tampFile = []\n",
    "        for sentence in file:\n",
    "            tampFile.append(associativeDictionary[types][\"test\"][j].doc2bow(sentence))\n",
    "        bowDict[types][\"test\"].append(tampFile)\n",
    "        \n",
    "    i+=1\n",
    "    j+=1\n",
    "print(associativeDictionary[\"K_NODE;cATERTETANCHGAZ\"][\"data\"][0].token2id)\n",
    "print(bowDict[\"K_NODE;cATERTETANCHGAZ\"][\"data\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "573e10ee-8ed1-4057-a822-7dd94fb71f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel<num_docs=2, num_nnz=69>\n"
     ]
    }
   ],
   "source": [
    "tfidfDict= {}\n",
    "\n",
    "for types in bowDict:\n",
    "\n",
    "    for file in bowDict[types][\"data\"]:\n",
    "        \n",
    "        if types in tfidfDict:\n",
    "            tfidfDict[types][\"data\"].append(gensim.models.TfidfModel(file))\n",
    "        else:\n",
    "            tfidfDict[types]={\"data\":[], \"test\":[]}\n",
    "            tfidfDict[types][\"data\"].append(gensim.models.TfidfModel(file))\n",
    "    \n",
    "    for file in bowDict[types][\"test\"]:\n",
    "        \n",
    "        tfidfDict[types][\"test\"].append(gensim.models.TfidfModel(file))\n",
    "        \n",
    "print(tfidfDict[\"K_NODE;cATERTETANCHGAZ\"][\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a260999-5d93-422f-b1e6-587f79e6a81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel<num_docs=5, num_nnz=160>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "de9487b3-e5ce-49c4-9bad-23ea6a9a02f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_NODE;cATERTETANCHGAZ\n",
      "Dictionary<55 unique tokens: ['.', 'client', 'contrôle', 'date', 'de']...>\n",
      "___________________________\n",
      "Dictionary<75 unique tokens: ['-corps', '-des', '-pierre', '.', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<59 unique tokens: ['.', 'adresse', 'affaire', 'avoir', 'certificat']...>\n",
      "___________________________\n",
      "Dictionary<55 unique tokens: ['.', 'adresse', 'affaire', 'avoir', 'certificat']...>\n",
      "___________________________\n",
      "Dictionary<59 unique tokens: ['-aubin', '-saint', '.', 'abdelouahid', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<63 unique tokens: ['.', 'client', 'contrôle', 'date', 'de']...>\n",
      "___________________________\n",
      "Dictionary<58 unique tokens: ['.', 'adresse', 'affaire', 'avoir', 'bessaque']...>\n",
      "___________________________\n",
      "Dictionary<78 unique tokens: ['-corps', '-des', '-pierre', '.', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<60 unique tokens: ['.', 'client', 'contrôle', 'date', 'de']...>\n",
      "___________________________\n",
      "Dictionary<72 unique tokens: ['-corps', '-des', '-pierre', '.', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<68 unique tokens: ['.', 'adresse', 'affaire', 'avoir', 'brigade']...>\n",
      "___________________________\n",
      "Dictionary<61 unique tokens: ['-corps', '-des', '-pierre', '.', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<63 unique tokens: ['.', 'adresse', 'affaire', 'avoir', 'bessaque']...>\n",
      "___________________________\n",
      "Dictionary<56 unique tokens: ['.', 'adresse', 'affaire', 'avoir', 'certificat']...>\n",
      "___________________________\n",
      "Dictionary<70 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<59 unique tokens: ['-aignan', '.', 'adresse', 'affaire', 'avoir']...>\n",
      "___________________________\n",
      "Dictionary<62 unique tokens: ['.', 'client', 'contrôle', 'date', 'de']...>\n",
      "___________________________\n",
      "Dictionary<58 unique tokens: ['.', 'abatelier', 'abdelouahid', 'administratif', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<61 unique tokens: ['-aignan', '.', 'adresse', 'affaire', 'avoir']...>\n",
      "___________________________\n",
      "Dictionary<60 unique tokens: ['-gare', '.', 'adresse', 'affaire', 'avoir']...>\n",
      "___________________________\n",
      "Dictionary<58 unique tokens: ['.', 'adresse', 'affaire', 'avoir', 'avord']...>\n",
      "___________________________\n",
      "Dictionary<62 unique tokens: ['-sur', '-vernisson', '.', 'adresse', 'affaire']...>\n",
      "___________________________\n",
      "Dictionary<57 unique tokens: ['-aubin', '-saint', '.', 'adresse', 'affaire']...>\n",
      "___________________________\n",
      "Dictionary<54 unique tokens: ['.', 'abatelier', 'abdelouahid', 'administratif', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<58 unique tokens: ['.', 'adresse', 'affaire', 'avoir', 'bedin']...>\n",
      "___________________________\n",
      "Dictionary<60 unique tokens: ['-gare', '.', 'adresse', 'affaire', 'avoir']...>\n",
      "___________________________\n",
      "Dictionary<55 unique tokens: ['.', 'adresse', 'affaire', 'avoir', 'certificat']...>\n",
      "___________________________\n",
      "Dictionary<71 unique tokens: ['.', 'adresse', 'affaire', 'avoir', 'cbmaintenance']...>\n",
      "___________________________\n",
      "Dictionary<61 unique tokens: ['-les', '-tours', '.', 'adresse', 'affaire']...>\n",
      "___________________________\n",
      "Dictionary<60 unique tokens: ['.', 'client', 'contrôle', 'date', 'de']...>\n",
      "___________________________\n",
      "Dictionary<18 unique tokens: ['client', 'contrôle', 'date', 'de', 'dintervention']...>\n",
      "___________________________\n",
      "Dictionary<59 unique tokens: ['-sur', '-vernisson', '.', 'abdelouahid', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<73 unique tokens: ['-corps', '-des', '-pierre', '.', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<58 unique tokens: ['.', 'abb.v', 'abdelouahid', 'adresse', 'affaire']...>\n",
      "___________________________\n",
      "Dictionary<61 unique tokens: ['-cher', '-sur', '.', 'adresse', 'affaire']...>\n",
      "___________________________\n",
      "Dictionary<60 unique tokens: ['-', '.', 'adresse', 'affaire', 'avoir']...>\n",
      "___________________________\n",
      "Dictionary<57 unique tokens: ['.', 'client', 'contrôle', 'date', 'de']...>\n",
      "___________________________\n",
      "Dictionary<59 unique tokens: ['-les', '-tours', '.', 'adresse', 'affaire']...>\n",
      "___________________________\n",
      "Dictionary<60 unique tokens: ['.', 'client', 'contrôle', 'date', 'de']...>\n",
      "___________________________\n",
      "Dictionary<58 unique tokens: ['.', 'adresse', 'affaire', 'avoir', 'certificat']...>\n",
      "___________________________\n",
      "K_NODE;cATERTRAMON\n",
      "Dictionary<63 unique tokens: ['-', '.', 'abdelouahiddaoudi', 'absorbé', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<78 unique tokens: ['.', 'client', 'date', 'de', 'dintervention']...>\n",
      "___________________________\n",
      "Dictionary<80 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<81 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<89 unique tokens: ['-', '-avant', '-saint', '.', 'ablocal']...>\n",
      "___________________________\n",
      "Dictionary<81 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<81 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<81 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<80 unique tokens: ['-', '.', 'adresse', 'affaire', 'caractéristique']...>\n",
      "___________________________\n",
      "Dictionary<64 unique tokens: ['-', '.', 'abdelouahiddaoudi', 'absorbé', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<70 unique tokens: ['-', '-creuse', '-gare', '-sur', '.']...>\n",
      "___________________________\n",
      "Dictionary<78 unique tokens: ['-', '.', 'absorbé', 'adresse', 'affaire']...>\n",
      "___________________________\n",
      "Dictionary<69 unique tokens: ['-', '.', 'abdelouahiddaoudi', 'absorbé', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<79 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<64 unique tokens: ['-', '.', 'abdelouahiddaoudi', 'absorbé', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<78 unique tokens: ['-', '.', 'absorbé', 'adresse', 'affaire']...>\n",
      "___________________________\n",
      "Dictionary<79 unique tokens: ['-', '.', 'absorbé', 'adresse', 'affaire']...>\n",
      "___________________________\n",
      "Dictionary<79 unique tokens: ['-', '-cher', '-sur', '.', 'absorbé']...>\n",
      "___________________________\n",
      "Dictionary<63 unique tokens: ['-', '.', 'a', 'abdelouahiddaoudi', 'absorbé']...>\n",
      "___________________________\n",
      "Dictionary<36 unique tokens: ['-', '.bedin', 'absorbé', 'bedin', 'bedincertificat']...>\n",
      "___________________________\n",
      "Dictionary<83 unique tokens: ['-', '.', 'absorbé', 'adresse', 'affaire']...>\n",
      "___________________________\n",
      "Dictionary<78 unique tokens: ['-', '-aignan', '.', 'absorbé', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<62 unique tokens: ['-', '.', 'absorbé', 'adresse', 'affaire']...>\n",
      "___________________________\n",
      "Dictionary<79 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<81 unique tokens: ['-', '.', 'absorbé', 'adresse', 'affaire']...>\n",
      "___________________________\n",
      "Dictionary<68 unique tokens: ['-', '-loire', '-sur', '.', 'abdelouahiddaoudi']...>\n",
      "___________________________\n",
      "Dictionary<79 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<88 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<81 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<78 unique tokens: ['-', '.', '.ii', 'absorbé', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<82 unique tokens: ['-', '.', 'absorbé', 'adresse', 'affaire']...>\n",
      "___________________________\n",
      "Dictionary<60 unique tokens: ['-', '.', 'a', 'abdelouahiddaoudi', 'absorbé']...>\n",
      "___________________________\n",
      "Dictionary<78 unique tokens: ['-', '.', 'absorbé', 'adresse', 'affaire']...>\n",
      "___________________________\n",
      "Dictionary<78 unique tokens: ['-', '.', 'absorbé', 'adresse', 'affaire']...>\n",
      "___________________________\n",
      "Dictionary<81 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<79 unique tokens: ['-', '-beuvron', '.', 'absorbé', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<70 unique tokens: ['-', '.', 'adresse', 'affaire', 'avenue']...>\n",
      "___________________________\n",
      "Dictionary<81 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<79 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<84 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<80 unique tokens: ['-', '-lanthenay', '.', 'abbv', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<30 unique tokens: ['-', 'absorbénom', 'be', 'bruno', 'caractéristique']...>\n",
      "___________________________\n",
      "Dictionary<68 unique tokens: ['.', 'absorbé', 'adresse', 'affaire', 'alexandrefoures']...>\n",
      "___________________________\n",
      "Dictionary<81 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<79 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<78 unique tokens: ['-', '-aignan', '.', 'absorbé', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<75 unique tokens: ['-', '.', 'absorbé', 'adresse', 'affaire']...>\n",
      "___________________________\n",
      "Dictionary<87 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<84 unique tokens: ['-', '-beuvron', '-m', '.', 'absorbé']...>\n",
      "___________________________\n",
      "Dictionary<56 unique tokens: ['.', 'client', 'contrôle', 'date', 'de']...>\n",
      "___________________________\n",
      "Dictionary<79 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<67 unique tokens: ['.', 'absorbé', 'adresse', 'affaire', 'alexandrefoures']...>\n",
      "___________________________\n",
      "Dictionary<87 unique tokens: ['-', '-e', '.', 'absorbé', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<81 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<62 unique tokens: ['-', '.', 'absorbé', 'adresse', 'affaire']...>\n",
      "___________________________\n",
      "Dictionary<54 unique tokens: ['-', '.', 'adresse', 'affaire', 'certificat']...>\n",
      "___________________________\n",
      "Dictionary<87 unique tokens: ['-', '-avant', '-saint', '.', 'abbv']...>\n",
      "___________________________\n",
      "Dictionary<56 unique tokens: ['.', 'adresse', 'affaire', 'bessaque', 'bourges']...>\n",
      "___________________________\n",
      "Dictionary<60 unique tokens: ['-', '.', 'a', 'absorbé', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<86 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<88 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<37 unique tokens: ['-', 'absorbé', 'bedincertificat', 'bruno', 'caractéristique']...>\n",
      "___________________________\n",
      "Dictionary<81 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<85 unique tokens: ['-', '-cher', '-sur', '.', 'absorbé']...>\n",
      "___________________________\n",
      "Dictionary<84 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<79 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<66 unique tokens: ['-', '-gare', '.', 'absorbé', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<51 unique tokens: ['.', 'adresse', 'affaire', 'amand', 'avoir']...>\n",
      "___________________________\n",
      "Dictionary<85 unique tokens: ['-', '-corps', '-des', '-pierre', '.']...>\n",
      "___________________________\n",
      "Dictionary<79 unique tokens: ['-', '-lanthenay', '.', 'abbv', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<93 unique tokens: ['-', 'absorbédonnée', 'affaire', 'au', 'avoir']...>\n",
      "___________________________\n",
      "Dictionary<81 unique tokens: ['-', '.', 'absorbé', 'adresse', 'affaire']...>\n",
      "___________________________\n",
      "Dictionary<64 unique tokens: ['-', '.', 'abdelouahiddaoudi', 'absorbé', 'adresse']...>\n",
      "___________________________\n",
      "Dictionary<73 unique tokens: ['-', '.', 'absorbé', 'adresse', 'affaire']...>\n",
      "___________________________\n",
      "Dictionary<50 unique tokens: ['.', 'adresse', 'affaire', 'avoir', 'bessaque']...>\n",
      "___________________________\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 50 is out of bounds for axis 0 with size 50",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [67]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m___________________________\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m types \u001b[38;5;129;01min\u001b[39;00m simsDict:\n\u001b[0;32m---> 11\u001b[0m     simsDict[types][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mgensim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSimilarity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mworkdir/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbowDict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43massociativeDictionary\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     simsDict[types] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m:[], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m:[]}\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/knitiv/lib/python3.10/site-packages/gensim/similarities/docsim.py:355\u001b[0m, in \u001b[0;36mSimilarity.__init__\u001b[0;34m(self, output_prefix, corpus, num_features, num_best, chunksize, shardsize, norm)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfresh_docs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfresh_nnz \u001b[38;5;241m=\u001b[39m [], \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/knitiv/lib/python3.10/site-packages/gensim/similarities/docsim.py:409\u001b[0m, in \u001b[0;36mSimilarity.add_documents\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m    407\u001b[0m         doc \u001b[38;5;241m=\u001b[39m matutils\u001b[38;5;241m.\u001b[39munitvec(matutils\u001b[38;5;241m.\u001b[39mcorpus2csc([doc], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features)\u001b[38;5;241m.\u001b[39mT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm)\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m         doc \u001b[38;5;241m=\u001b[39m matutils\u001b[38;5;241m.\u001b[39munitvec(\u001b[43mmatutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse2full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_features\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm)\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfresh_docs\u001b[38;5;241m.\u001b[39mappend(doc)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfresh_nnz \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m doclen\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/knitiv/lib/python3.10/site-packages/gensim/matutils.py:398\u001b[0m, in \u001b[0;36msparse2full\u001b[0;34m(doc, length)\u001b[0m\n\u001b[1;32m    396\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(doc)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# overwrite some of the zeroes with explicit values\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m result[\u001b[38;5;28mlist\u001b[39m(doc)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(doc\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mIndexError\u001b[0m: index 50 is out of bounds for axis 0 with size 50"
     ]
    }
   ],
   "source": [
    "simsDict = {}\n",
    "bowTest= bowDict[\"K_NODE;cATERTETANCHGAZ\"][\"data\"]\n",
    "for types in tfidfDict: \n",
    "    j=0\n",
    "    i=0\n",
    "    print(types)\n",
    "    for file in tfidfDict[types][\"data\"]:\n",
    "        print(associativeDictionary[types][\"data\"][i])\n",
    "        print(\"___________________________\")\n",
    "        if types in simsDict:\n",
    "            simsDict[types][\"data\"].append(gensim.similarities.Similarity('workdir/',file[bowDict[types][\"data\"][i]],num_features=len(associativeDictionary[types][\"data\"][i])))\n",
    "        else:\n",
    "            simsDict[types] = {\"data\":[], \"test\":[]}\n",
    "            tfidf = tfidfDict[\"K_NODE;cATERTDEBAIR\"][\"data\"]\n",
    "            simsDict[types][\"data\"].append(gensim.similarities.Similarity('workdir/',file[bowDict[types][\"data\"][i]],num_features=len(associativeDictionary[types][\"data\"][i])))\n",
    "        i+=1\n",
    "        \n",
    "    for file in tfidfDict[types][\"test\"]:\n",
    "        simsDict[types][\"test\"].append(gensim.similarities.Similarity('workdir/',file[bowTest[j]],num_features=len(associativeDictionary[types][\"test\"][j])))\n",
    "        \n",
    "        j+=1\n",
    "print(simsDict[\"K_NODE;cATERTETANCHGAZ\"][\"test\"][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f2f72074-0df2-4439-b5fd-240dbc925725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 3), (8, 1), (9, 1), (10, 1), (11, 2), (12, 1), (13, 1), (14, 1), (15, 1), (16, 2), (17, 1), (18, 1), (19, 1), (20, 3), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (29, 1), (30, 2), (31, 1), (32, 1), (33, 1), (34, 2)], [(0, 1), (2, 1), (7, 14), (8, 3), (9, 3), (11, 5), (13, 5), (14, 3), (15, 1), (19, 1), (20, 1), (22, 1), (24, 1), (26, 1), (28, 2), (29, 6), (33, 1), (34, 1), (35, 9), (36, 1), (37, 3), (38, 3), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 2), (46, 2), (47, 1), (48, 1), (49, 2), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 2), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 2), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 3), (76, 2), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 2), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 4), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 2), (110, 1), (111, 1), (112, 5), (113, 2), (114, 1), (115, 8), (116, 1), (117, 1), (118, 2), (119, 2), (120, 1), (121, 1), (122, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 2), (142, 1), (143, 2), (144, 1), (145, 1)], [(0, 1), (2, 1), (7, 6), (9, 1), (14, 1), (15, 2), (19, 3), (34, 1), (35, 4), (36, 1), (42, 1), (44, 1), (64, 2), (67, 1), (75, 2), (82, 1), (99, 2), (112, 6), (115, 6), (119, 1), (120, 2), (146, 1), (147, 1), (148, 1), (149, 1), (150, 1), (151, 1), (152, 1), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 2), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1), (175, 2), (176, 1), (177, 2), (178, 1), (179, 1), (180, 1), (181, 1), (182, 1), (183, 1), (184, 1), (185, 1)], [(0, 1), (7, 2), (15, 1), (19, 1), (159, 2), (186, 1), (187, 1), (188, 1), (189, 1), (190, 1), (191, 1)], [(0, 1), (34, 1), (76, 1), (159, 1), (192, 1), (193, 1), (194, 1), (195, 1), (196, 1), (197, 3), (198, 1), (199, 2), (200, 1), (201, 1), (202, 1), (203, 1), (204, 1), (205, 1)], [(0, 1), (7, 1), (11, 1), (15, 4), (19, 1), (20, 1), (35, 2), (67, 1), (88, 2), (113, 1), (137, 1), (149, 1), (162, 1), (164, 1), (174, 1), (184, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (211, 1), (212, 1), (213, 1), (214, 2), (215, 1), (216, 1), (217, 1), (218, 4), (219, 1)], [(0, 1), (6, 1), (14, 1), (19, 1), (67, 1), (159, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 1), (225, 1)], [(0, 1), (24, 1), (42, 1), (226, 1), (227, 1), (228, 1), (229, 1)]]\n"
     ]
    }
   ],
   "source": [
    "def setupFileToCompare(file):\n",
    "    \n",
    "    #documentTest = data[\"K_NODE;cATERTDISCO\"][\"test\"][13]\n",
    "    sents = sent_tokenize(file)\n",
    "    wordTest=[]\n",
    "    for sent in sents: \n",
    "        wordTest.append(word_tokenize(sent))\n",
    "    dictDocTest = gensim.corpora.Dictionary(wordTest)\n",
    "    bowDocTest = []\n",
    "    for sentence in wordTest:\n",
    "        bowDocTest.append(dictDocTest.doc2bow(sentence))\n",
    "    return bowDocTest\n",
    "\n",
    "bowFile = setupFileToCompare(data[\"K_NODE;cATERTDISCO\"][\"test\"][13])\n",
    "print(bowFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c4ac5f7-e7ec-4568-945b-3fd1135b2078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity<3 documents in 0 shards stored under workdir/>\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'workdir/.0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/knitiv/lib/python3.10/site-packages/gensim/utils.py:764\u001b[0m, in \u001b[0;36mSaveLoad.save\u001b[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 764\u001b[0m     \u001b[43m_pickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m object\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: file must have a 'write' attribute",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sims \u001b[38;5;129;01min\u001b[39;00m simsDict[types][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sims)\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43msims\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbowFile\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/knitiv/lib/python3.10/site-packages/gensim/similarities/docsim.py:536\u001b[0m, in \u001b[0;36mSimilarity.__getitem__\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, query):\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;124;03m\"\"\"Get similarities of the document (or corpus) `query` to all documents in the corpus.\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \n\u001b[1;32m    507\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    534\u001b[0m \n\u001b[1;32m    535\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose_shard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# no-op if no documents added to index since last query\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;66;03m# reset num_best and normalize parameters, in case they were changed dynamically\u001b[39;00m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m shard \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshards:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/knitiv/lib/python3.10/site-packages/gensim/similarities/docsim.py:459\u001b[0m, in \u001b[0;36mSimilarity.close_shard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m     index \u001b[38;5;241m=\u001b[39m MatrixSimilarity(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfresh_docs, num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features)\n\u001b[1;32m    458\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreating \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m shard #\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m'\u001b[39m, shardid)\n\u001b[0;32m--> 459\u001b[0m shard \u001b[38;5;241m=\u001b[39m \u001b[43mShard\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshardid2filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshardid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m shard\u001b[38;5;241m.\u001b[39mnum_best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_best\n\u001b[1;32m    461\u001b[0m shard\u001b[38;5;241m.\u001b[39mnum_nnz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfresh_nnz\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/knitiv/lib/python3.10/site-packages/gensim/similarities/docsim.py:116\u001b[0m, in \u001b[0;36mShard.__init__\u001b[0;34m(self, fname, index)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[1;32m    115\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaving index shard to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfullname())\n\u001b[0;32m--> 116\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfullname\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index()\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/knitiv/lib/python3.10/site-packages/gensim/utils.py:767\u001b[0m, in \u001b[0;36mSaveLoad.save\u001b[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    765\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m object\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `fname_or_handle` does not have write attribute\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_smart_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparately\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/knitiv/lib/python3.10/site-packages/gensim/utils.py:611\u001b[0m, in \u001b[0;36mSaveLoad._smart_save\u001b[0;34m(self, fname, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    607\u001b[0m restores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_specials(\n\u001b[1;32m    608\u001b[0m     fname, separately, sep_limit, ignore, pickle_protocol, compress, subname,\n\u001b[1;32m    609\u001b[0m )\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 611\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;66;03m# restore attribs handled specially\u001b[39;00m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj, asides \u001b[38;5;129;01min\u001b[39;00m restores:\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/knitiv/lib/python3.10/site-packages/gensim/utils.py:1442\u001b[0m, in \u001b[0;36mpickle\u001b[0;34m(obj, fname, protocol)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpickle\u001b[39m(obj, fname, protocol\u001b[38;5;241m=\u001b[39mPICKLE_PROTOCOL):\n\u001b[1;32m   1430\u001b[0m     \u001b[38;5;124;03m\"\"\"Pickle object `obj` to file `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.\u001b[39;00m\n\u001b[1;32m   1431\u001b[0m \n\u001b[1;32m   1432\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \n\u001b[1;32m   1441\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fout:  \u001b[38;5;66;03m# 'b' for binary, needed on Windows\u001b[39;00m\n\u001b[1;32m   1443\u001b[0m         _pickle\u001b[38;5;241m.\u001b[39mdump(obj, fout, protocol\u001b[38;5;241m=\u001b[39mprotocol)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/knitiv/lib/python3.10/site-packages/smart_open/smart_open_lib.py:177\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transport_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     transport_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 177\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43m_shortcut_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/knitiv/lib/python3.10/site-packages/smart_open/smart_open_lib.py:363\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    361\u001b[0m     open_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m errors\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_builtin_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'workdir/.0'"
     ]
    }
   ],
   "source": [
    "#try to have a average about document types similaritys\n",
    "simsList=[]\n",
    "docSimilarity=[]\n",
    "for types in simsDict:\n",
    "    for sims in simsDict[types][\"data\"]:\n",
    "        print(sims)\n",
    "        print(sims[bowFile])\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213260a9-c439-4fa7-81e3-87e709358054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
